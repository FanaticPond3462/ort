---
title: Introduction
---

<p style={{ fontSize: '1.5rem', textAlign: 'center' }}>
    `ort` is an open-source Rust binding for [ONNX Runtime](https://onnxruntime.ai/).
</p>

`ort` makes it easy to deploy your machine learning models to production via [ONNX Runtime](https://onnxruntime.ai/), a hardware-accelerated inference engine. With `ort` + ONNX Runtime, you can run almost any ML model (including ResNet, YOLOv8, BERT, LLaMA) on almost any hardware, often far faster than PyTorch, and with the added bonus of Rust's efficiency.

<Warning>
    These docs are for the latest alpha version of `ort`, `2.0.0-rc.1`. This version is production-ready (just not API stable) and we recommend new & existing projects use it.
</Warning>

# Why `ort`?
There are a few other ONNX Runtime crates out there, so why use `ort`?

For one, `ort` simply supports more features:

| Feature comparison        | **üìï ort** | **üìó [ors](https://github.com/HaoboGu/ors)** | **ü™ü [onnxruntime-rs](https://github.com/microsoft/onnxruntime/tree/main/rust)** |
|---------------------------|-----------|-----------|----------------------|
| Upstream version          | **v1.17.1** | v1.12.0 | v1.8               |
| `dlopen()`?               | ‚úÖ         | ‚úÖ         | ‚ùå                    |
| Execution providers?      | ‚úÖ         | ‚ùå         | ‚ùå                    |
| I/O Binding?              | ‚úÖ         | ‚ùå         | ‚ùå                    |
| String tensors?           | ‚úÖ         | ‚ùå         | ‚ö†Ô∏è input only         |
| Multiple output types?    | ‚úÖ         | ‚úÖ         | ‚ùå                    |
| Multiple input types?     | ‚úÖ         | ‚úÖ         | ‚ùå                    |
| In-memory session?        | ‚úÖ         | ‚úÖ         | ‚úÖ                    |
| WebAssembly?              | ‚úÖ         | ‚ùå         | ‚ùå                    |
| Provides static binaries? | ‚úÖ         | ‚ùå         | ‚ùå                    |
| Sequence & map types?     | ‚úÖ         | ‚ùå         | ‚ùå                    |

Users of `ort` appreciate its ease of use and ergonomic API. `ort` is also battle tested in some pretty serious production scenarios.
- [**Twitter**](https://twitter.com/) uses `ort` in part of their recommendations system, serving hundreds of millions of requests a day.
- [**Bloop**](https://bloop.ai/)'s semantic code search feature is powered by `ort`.
- [**SurrealDB**](https://surrealdb.com/) uses `ort` in their [`surrealml`](https://github.com/surrealdb/surrealml) package.
- [**Numerical Elixir**](https://github.com/elixir-nx) uses `ort` to create ONNX Runtime bindings for the Elixir language.
- [**`rust-bert`**](https://github.com/guillaume-be/rust-bert) implements many ready-to-use NLP pipelines in Rust √† la Hugging Face Transformers with both [`tch`](https://crates.io/crates/tch) & `ort` backends.
- [**`edge-transformers`**](https://github.com/npc-engine/edge-transformers) also implements Hugging Face Transformers pipelines in Rust using `ort`.

# Getting started
<Steps>
    <Step title="Add ort to your Cargo.toml">
        If you have a [supported platform](/setup/platforms) (and you probably do), installing `ort` couldn't be any simpler! Just add it to your Cargo dependencies:
        ```toml
        [dependencies]
        ort = "2.0.0-rc.1"
        ```
    </Step>
    <Step title="Convert your model">
        Your model will need to be converted to the [ONNX](https://onnx.ai/) format before you can use it.
        - The awesome folks at Hugging Face have [a guide](https://huggingface.co/docs/transformers/serialization) to export ü§ó Transformers models to ONNX with ü§ó Optimum.
        - For other PyTorch models: [`torch.onnx`](https://pytorch.org/docs/stable/onnx.html)
        - For `scikit-learn`: [`sklearn-onnx`](https://onnx.ai/sklearn-onnx/)
        - For TensorFlow, Keras, TFlite, TensorFlow.js: [`tf2onnx`](https://github.com/onnx/tensorflow-onnx)
        - For PaddlePaddle: [`Paddle2ONNX`](https://github.com/PaddlePaddle/Paddle2ONNX)
    </Step>
    <Step title="Load your model">
        Once you've got a model, load it via `ort` by creating a [`Session`](/fundamentals/session):

        ```rust
        use ort::{GraphOptimizationLevel, Session};

        let model = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .with_intra_threads(4)?
            .commit_from_file("yolov8m.onnx")?;
        ```
    </Step>
    <Step title="Perform inference">
        Preprocess your inputs, then `run()` the session to perform inference.

        ```rust
        let outputs = model.run(ort::inputs!["image" => image]?)?;

        // Postprocessing
        let output = outputs["output0"]
            .extract_tensor::<f32>()?
            .t()
            .slice(s![.., .., 0])
            .into_owned();
        ...
        ```

        <Note>There are some more useful examples [in the `ort` repo](https://github.com/pykeio/ort/tree/main/examples)!</Note>
    </Step>
</Steps>

# Next steps
<Steps>
    <Step title="Unlock more performance with EPs">
        Use [execution providers](/perf/execution-providers) to enable hardware acceleration in your app and unlock the full power of your GPU or NPU.
    </Step>
    <Step title="Show off your project!">
        We'd love to see what you've made with `ort`! Show off your project in [GitHub Discussions](https://github.com/pykeio/ort/discussions/categories/show-and-tell) or on our [Discord](https://discord.gg/uQtsNu2xMa).
    </Step>
</Steps>
